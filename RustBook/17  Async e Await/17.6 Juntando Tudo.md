# Juntando Tudo: Futures, Tarefas e Threads

Como vimos no Capítulo 16, threads fornecem uma abordagem para concorrência. Neste capítulo, vimos outra abordagem: usar async com futures e streams. Se você está se perguntando quando escolher um método em vez do outro, a resposta é: depende! E em muitos casos, a escolha não é threads _ou_ async, mas sim threads _e_ async.

Muitos sistemas operacionais fornecem modelos de concorrência baseados em threads há décadas, e muitas linguagens de programação os suportam como resultado. No entanto, esses modelos não são sem seus trade-offs. Em muitos sistemas operacionais, eles usam uma boa quantidade de memória para cada thread, e vêm com alguma sobrecarga para iniciar e encerrar. Threads também são apenas uma opção quando seu sistema operacional e hardware os suportam. Diferentemente dos computadores desktop e móveis convencionais, alguns sistemas embarcados não têm um sistema operacional, então também não têm threads.

O modelo async fornece um conjunto diferente—e complementar—de trade-offs. No modelo async, operações concorrentes não exigem suas próprias threads. Em vez disso, elas podem ser executadas em tarefas, como quando usamos `trpl::spawn_task` para iniciar trabalho a partir de uma função síncrona na seção de streams. Uma tarefa é semelhante a uma thread, mas em vez de ser gerenciada pelo sistema operacional, é gerenciada por código em nível de biblioteca: o runtime.

Na seção anterior, vimos que poderíamos construir um stream usando um canal async e gerando uma tarefa async que poderíamos chamar a partir de código síncrono. Podemos fazer exatamente a mesma coisa com uma thread. Na Listagem 17-40, usamos `trpl::spawn_task` e `trpl::sleep`. Na Listagem 17-41, substituímos essas funções pelas APIs `thread::spawn` e `thread::sleep` da biblioteca padrão na função `get_intervals`.

Nome do arquivo: src/main.rs

```rust
extern crate trpl; // necessário para mdbook test

use std::{pin::pin, thread, time::Duration};

use trpl::{ReceiverStream, Stream, StreamExt};

fn main() {
    trpl::run(async {
        let messages = get_messages().timeout(Duration::from_millis(200));
        let intervals = get_intervals()
            .map(|count| format!("Intervalo #{count}"))
            .throttle(Duration::from_millis(500))
            .timeout(Duration::from_secs(10));
        let merged = messages.merge(intervals).take(20);
        let mut stream = pin!(merged);

        while let Some(result) = stream.next().await {
            match result {
                Ok(item) => println!("{item}"),
                Err(reason) => eprintln!("Problema: {reason:?}"),
            }
        }
    });
}

fn get_messages() -> impl Stream<Item = String> {
    let (tx, rx) = trpl::channel();

    trpl::spawn_task(async move {
        let messages = ["a", "b", "c", "d", "e", "f", "g", "h", "i", "j"];

        for (index, message) in messages.into_iter().enumerate() {
            let time_to_sleep = if index % 2 == 0 { 100 } else { 300 };
            trpl::sleep(Duration::from_millis(time_to_sleep)).await;

            if let Err(send_error) = tx.send(format!("Mensagem: '{message}'")) {
                eprintln!("Não foi possível enviar mensagem '{message}': {send_error}");
                break;
            }
        }
    });

    ReceiverStream::new(rx)
}

fn get_intervals() -> impl Stream<Item = u32> {
    let (tx, rx) = trpl::channel();

    // Isto *não* é `trpl::spawn` mas `std::thread::spawn`!
    thread::spawn(move || {
        let mut count = 0;
        loop {
            // Da mesma forma, isto *não* é `trpl::sleep` mas `std::thread::sleep`!
            thread::sleep(Duration::from_millis(1));
            count += 1;

            if let Err(send_error) = tx.send(count) {
                eprintln!("Não foi possível enviar intervalo {count}: {send_error}");
                break;
            };
        }
    });

    ReceiverStream::new(rx)
}
```

Listagem 17-41: Usando as APIs `std::thread` em vez das APIs async `trpl` para a função `get_intervals`

Se você executar este código, a saída é idêntica à da Listagem 17-40. E observe como pouco muda aqui da perspectiva do código de chamada. Além disso, mesmo que uma de nossas funções tenha gerado uma tarefa async no runtime e a outra tenha gerado uma thread do sistema operacional, os streams resultantes não foram afetados pelas diferenças.

Apesar de suas semelhanças, essas duas abordagens se comportam de maneira muito diferente, embora possamos ter dificuldade em medi-las neste exemplo muito simples. Poderíamos gerar milhões de tarefas async em qualquer computador pessoal moderno. Se tentássemos fazer isso com threads, literalmente ficaríamos sem memória!

No entanto, há uma razão para essas APIs serem tão semelhantes. Threads atuam como um limite para conjuntos de operações síncronas; a concorrência é possível _entre_ threads. Tarefas atuam como um limite para conjuntos de operações _assíncronas_; a concorrência é possível tanto _entre_ quanto _dentro_ de tarefas, porque uma tarefa pode alternar entre futures em seu corpo. Finalmente, futures são a unidade mais granular de concorrência do Rust, e cada future pode representar uma árvore de outras futures. O runtime—especificamente, seu executor—gerencia tarefas, e tarefas gerenciam futures. Nesse sentido, tarefas são semelhantes a threads leves gerenciadas pelo runtime com capacidades adicionais que vêm de serem gerenciadas por um runtime em vez de pelo sistema operacional.

Isso não significa que tarefas async sejam sempre melhores que threads (ou vice-versa). A concorrência com threads é, em alguns aspectos, um modelo de programação mais simples do que a concorrência com `async`. Isso pode ser uma força ou uma fraqueza. Threads são de certa forma "dispare e esqueça"; elas não têm equivalente nativo a uma future, então simplesmente executam até a conclusão sem serem interrompidas, exceto pelo próprio sistema operacional. Ou seja, elas não têm suporte integrado para _concorrência intratarefa_ como as futures têm. Threads em Rust também não têm mecanismos para cancelamento—um assunto que não cobrimos explicitamente neste capítulo, mas que foi implícito pelo fato de que sempre que encerrávamos uma future, seu estado era limpo corretamente.

Essas limitações também tornam as threads mais difíceis de compor do que as futures. É muito mais difícil, por exemplo, usar threads para construir auxiliares como os métodos `timeout` e `throttle` que construímos anteriormente neste capítulo. O fato de as futures serem estruturas de dados mais ricas significa que elas podem ser compostas de forma mais natural, como vimos.

As tarefas, então, nos dão controle _adicional_ sobre as futures, permitindo-nos escolher onde e como agrupá-las. E acontece que threads e tarefas frequentemente funcionam muito bem juntas, porque tarefas podem (pelo menos em alguns runtimes) ser movidas entre threads. De fato, sob o capô, o runtime que temos usado—incluindo as funções `spawn_blocking` e `spawn_task`—é multithread por padrão! Muitos runtimes usam uma abordagem chamada _roubo de trabalho_ (work stealing) para mover transparentemente tarefas entre threads, com base em como as threads estão sendo utilizadas atualmente, para melhorar o desempenho geral do sistema. Essa abordagem realmente requer threads _e_ tarefas e, portanto, futures.

Ao pensar sobre qual método usar quando, considere estas regras práticas:
- Se o trabalho é _muito paralelizável_, como processar um monte de dados onde cada parte pode ser processada separadamente, threads são uma escolha melhor.
- Se o trabalho é _muito concorrente_, como lidar com mensagens de várias fontes diferentes que podem chegar em intervalos ou taxas diferentes, async é uma escolha melhor.

E se você precisa de paralelismo e concorrência, não precisa escolher entre threads e async. Você pode usá-los juntos livremente, deixando cada um desempenhar o papel em que é melhor. Por exemplo, a Listagem 17-42 mostra um exemplo bastante comum desse tipo de mistura em código Rust do mundo real.

Nome do arquivo: src/main.rs

```rust
extern crate trpl; // for mdbook test

use std::{thread, time::Duration};

fn main() {
    let (tx, mut rx) = trpl::channel();

    thread::spawn(move || {
        for i in 1..11 {
            tx.send(i).unwrap();
            thread::sleep(Duration::from_secs(1));
        }
    });

    trpl::run(async {
        while let Some(message) = rx.recv().await {
            println!("{message}");
        }
    });
}
```

Listagem 17-42: Enviando mensagens com código bloqueante em uma thread e aguardando as mensagens em um bloco async

Começamos criando um canal async, depois geramos uma thread que assume a propriedade do lado remetente do canal. Dentro da thread, enviamos os números de 1 a 10, dormindo por um segundo entre cada um. Finalmente, executamos uma future criada com um bloco async passado para `trpl::run` como fizemos ao longo do capítulo. Nessa future, aguardamos essas mensagens, assim como nos outros exemplos de passagem de mensagens que vimos.

Para retornar ao cenário com o qual abrimos o capítulo, imagine executar um conjunto de tarefas de codificação de vídeo usando uma thread dedicada (porque a codificação de vídeo é vinculada à computação), mas notificando a UI de que essas operações foram concluídas com um canal async. Existem inúmeros exemplos desses tipos de combinações em casos de uso do mundo real.

## Resumo

Esta não é a última vez que você verá concorrência neste livro. O projeto no Capítulo 21 aplicará esses conceitos em uma situação mais realista do que os exemplos mais simples discutidos aqui e comparará a resolução de problemas com threading versus tarefas de forma mais direta.

Independentemente de qual dessas abordagens você escolher, Rust fornece as ferramentas necessárias para escrever código concorrente seguro e rápido—seja para um servidor web de alta taxa de transferência ou um sistema operacional embarcado.

A seguir, falaremos sobre maneiras idiomáticas de modelar problemas e estruturar soluções à medida que seus programas Rust ficam maiores. Além disso, discutiremos como os idiomas de Rust se relacionam com aqueles com os quais você pode estar familiarizado da programação orientada a objetos.
