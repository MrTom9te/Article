# Transformando Nosso Servidor de Thread Única em um Servidor Multithread

No momento, o servidor processará cada requisição em sequência, o que significa que ele não processará uma segunda conexão até que a primeira termine o processamento. Se o servidor recebesse mais e mais requisições, essa execução serial seria cada vez menos otimizada. Se o servidor receber uma requisição que demore muito para processar, as requisições subsequentes terão que esperar até que a requisição longa seja concluída, mesmo que as novas requisições possam ser processadas rapidamente. Precisaremos corrigir isso, mas primeiro, veremos o problema em ação.

### Simulando uma Requisição Lenta na Implementação Atual do Servidor

Vamos ver como uma requisição de processamento lento pode afetar outras requisições feitas à nossa implementação de servidor atual. A Listagem 21-10 implementa o tratamento de uma requisição para */sleep* com uma resposta lenta simulada que fará com que o servidor durma por 5 segundos antes de responder.

*Arquivo: src/main.rs*

```rust
use std::{
    fs,
    io::{prelude::*, BufReader},
    net::{TcpListener, TcpStream},
    thread,
    time::Duration,
};

// --snip--

fn main() {
    let listener = TcpListener::bind("127.0.0.1:7878").unwrap();

    for stream in listener.incoming() {
        let stream = stream.unwrap();

        handle_connection(stream);
    }
}

fn handle_connection(mut stream: TcpStream) {
    // --snip--
    let buf_reader = BufReader::new(&stream);
    let request_line = buf_reader.lines().next().unwrap().unwrap();

    let (status_line, filename) = match &request_line[..] {
        "GET / HTTP/1.1" => ("HTTP/1.1 200 OK", "hello.html"),
        "GET /sleep HTTP/1.1" => {
            thread::sleep(Duration::from_secs(5));
            ("HTTP/1.1 200 OK", "hello.html")
        }
        _ => ("HTTP/1.1 404 NOT FOUND", "404.html"),
    };

    // --snip--
    let contents = fs::read_to_string(filename).unwrap();
    let length = contents.len();

    let response =
        format!("{status_line}\r\nContent-Length: {length}\r\n\r\n{contents}");

    stream.write_all(response.as_bytes()).unwrap();
}
```

*Listagem 21-10: Simulando uma requisição lenta dormindo por 5 segundos*

Mudamos de `if` para `match` agora que temos três casos. Precisamos explicitamente fazer o match em uma fatia de `request_line` para fazer o pattern matching contra os valores literais de string; `match` não faz referenciação e desreferenciação automática como o método de igualdade faz.

O primeiro braço é o mesmo que o bloco `if` da Listagem 21-9. O segundo braço corresponde a uma requisição para */sleep*. Quando essa requisição é recebida, o servidor dormirá por 5 segundos antes de renderizar a página HTML de sucesso. O terceiro braço é o mesmo que o bloco `else` da Listagem 21-9.

Você pode ver como nosso servidor é primitivo: bibliotecas reais lidariam com o reconhecimento de múltiplas requisições de uma forma muito menos verbosa!

Inicie o servidor usando `cargo run`. Em seguida, abra duas janelas do navegador: uma para *http://127.0.0.1:7878/* e outra para *http://127.0.0.1:7878/sleep*. Se você inserir o URI */* algumas vezes, como antes, verá que ele responde rapidamente. Mas se você inserir */sleep* e, em seguida, carregar */*, verá que */* espera até que `sleep` tenha dormido por seus 5 segundos completos antes de carregar.

Existem várias técnicas que poderíamos usar para evitar que as requisições fiquem atrasadas por causa de uma requisição lenta, incluindo o uso de async como fizemos no Capítulo 17; a que implementaremos é um pool de threads.

### Melhorando a Vazão com um Pool de Threads

Um *pool de threads* é um grupo de threads geradas que estão esperando e prontas para lidar com uma tarefa. Quando o programa recebe uma nova tarefa, ele atribui uma das threads do pool à tarefa, e essa thread processará a tarefa. As threads restantes no pool estão disponíveis para lidar com quaisquer outras tarefas que cheguem enquanto a primeira thread estiver processando. Quando a primeira thread terminar de processar sua tarefa, ela será devolvida ao pool de threads ociosas, pronta para lidar com uma nova tarefa. Um pool de threads permite que você processe conexões simultaneamente, aumentando a vazão do seu servidor.

Limitaremos o número de threads no pool a um pequeno número para nos proteger de ataques de Negação de Serviço (DoS); se tivéssemos nosso programa criando uma nova thread para cada requisição conforme ela chegasse, alguém fazendo 10 milhões de requisições ao nosso servidor poderia criar um caos usando todos os recursos do nosso servidor e paralisando o processamento de requisições.

Em vez de gerar threads ilimitadas, então, teremos um número fixo de threads esperando no pool. As requisições que chegam são enviadas ao pool para processamento. O pool manterá uma fila de requisições de entrada. Cada uma das threads do pool retirará uma requisição dessa fila, lidará com a requisição e, em seguida, pedirá outra requisição à fila. Com esse design, podemos processar até `N` requisições simultaneamente, onde `N` é o número de threads. Se cada thread estiver respondendo a uma requisição de longa duração, as requisições subsequentes ainda podem ficar na fila, mas aumentamos o número de requisições de longa duração que podemos lidar antes de atingir esse ponto.

Essa técnica é apenas uma das muitas maneiras de melhorar a vazão de um servidor web. Outras opções que você pode explorar são o *modelo fork/join*, o *modelo de E/S assíncrona de thread única* ou o *modelo de E/S assíncrona multithread*. Se você estiver interessado neste tópico, pode ler mais sobre outras soluções e tentar implementá-las; com uma linguagem de baixo nível como Rust, todas essas opções são possíveis.

Antes de começarmos a implementar um pool de threads, vamos falar sobre como usar o pool deve ser. Quando você está tentando projetar código, escrever a interface do cliente primeiro pode ajudar a guiar seu design. Escreva a API do código para que ela seja estruturada da maneira que você deseja chamá-la; em seguida, implemente a funcionalidade dentro dessa estrutura, em vez de implementar a funcionalidade e depois projetar a API pública.

Semelhante a como usamos o desenvolvimento orientado a testes no projeto no Capítulo 12, usaremos o desenvolvimento orientado ao compilador aqui. Escreveremos o código que chama as funções que queremos e, em seguida, veremos os erros do compilador para determinar o que devemos mudar em seguida para fazer o código funcionar. Antes de fazermos isso, no entanto, exploraremos a técnica que *não* vamos usar como ponto de partida.

#### Gerando uma Thread para Cada Requisição

Primeiro, vamos explorar como nosso código poderia parecer se ele criasse uma nova thread para cada conexão. Conforme mencionado anteriormente, este não é nosso plano final devido aos problemas com a geração potencial de um número ilimitado de threads, mas é um ponto de partida para obter um servidor multithreaded funcionando primeiro. Em seguida, adicionaremos o pool de threads como uma melhoria, e contrastar as duas soluções será mais fácil. A Listagem 21-11 mostra as alterações a serem feitas em `main` para gerar uma nova thread para lidar com cada stream dentro do loop `for`.

*Arquivo: src/main.rs*

```rust
use std::{
    fs,
    io::{prelude::*, BufReader},
    net::{TcpListener, TcpStream},
    thread,
    time::Duration,
};

fn main() {
    let listener = TcpListener::bind("127.0.0.1:7878").unwrap();

    for stream in listener.incoming() {
        let stream = stream.unwrap();

        thread::spawn(|| {
            handle_connection(stream);
        });
    }
}
// --snip--

fn handle_connection(mut stream: TcpStream) {
    let buf_reader = BufReader::new(&stream);
    let request_line = buf_reader.lines().next().unwrap().unwrap();

    let (status_line, filename) = match &request_line[..] {
        "GET / HTTP/1.1" => ("HTTP/1.1 200 OK", "hello.html"),
        "GET /sleep HTTP/1.1" => {
            thread::sleep(Duration::from_secs(5));
            ("HTTP/1.1 200 OK", "hello.html")
        }
        _ => ("HTTP/1.1 404 NOT FOUND", "404.html"),
    };

    let contents = fs::read_to_string(filename).unwrap();
    let length = contents.len();

    let response =
        format!("{status_line}\r\nContent-Length: {length}\r\n\r\n{contents}");

    stream.write_all(response.as_bytes()).unwrap();
}
```

*Listagem 21-11: Gerando uma nova thread para cada stream*

Como você aprendeu no Capítulo 16, `thread::spawn` criará uma nova thread e, em seguida, executará o código na closure na nova thread. Se você executar este código e carregar */sleep* no seu navegador, e depois */* em mais duas abas do navegador, você verá que as requisições para */* não precisam esperar que */sleep* termine. No entanto, como mencionamos, isso eventualmente sobrecarregará o sistema porque você estaria criando novas threads sem nenhum limite.

Você também pode se lembrar do Capítulo 17 que este é exatamente o tipo de situação em que async e await realmente brilham! Tenha isso em mente enquanto construímos o pool de threads e pense em como as coisas pareceriam diferentes ou iguais com async.

#### Criando um Número Finito de Threads

Queremos que nosso pool de threads funcione de maneira semelhante e familiar para que a mudança de threads para um pool de threads não exija grandes alterações no código que usa nossa API. A Listagem 21-12 mostra a interface hipotética para uma estrutura `ThreadPool` que queremos usar em vez de `thread::spawn`.

*Arquivo: src/main.rs*

```rust
// --imports omitted--
use std::{
    fs,
    io::{prelude::*, BufReader},
    net::{TcpListener, TcpStream},
    thread,
    time::Duration,
};

fn main() {
    let listener = TcpListener::bind("127.0.0.1:7878").unwrap();
    let pool = ThreadPool::new(4);

    for stream in listener.incoming() {
        let stream = stream.unwrap();

        pool.execute(|| {
            handle_connection(stream);
        });
    }
}

fn handle_connection(mut stream: TcpStream) {
 let buf_reader = BufReader::new(&stream);
    let request_line = buf_reader.lines().next().unwrap().unwrap();

    let (status_line, filename) = match &request_line[..] {
        "GET / HTTP/1.1" => ("HTTP/1.1 200 OK", "hello.html"),
        "GET /sleep HTTP/1.1" => {
            thread::sleep(Duration::from_secs(5));
            ("HTTP/1.1 200 OK", "hello.html")
        }
        _ => ("HTTP/1.1 404 NOT FOUND", "404.html"),
    };

    let contents = fs::read_to_string(filename).unwrap();
    let length = contents.len();

    let response =
        format!("{status_line}\r\nContent-Length: {length}\r\n\r\n{contents}");

    stream.write_all(response.as_bytes()).unwrap();
}
```
*Listagem 21-12: Nossa interface ideal para `ThreadPool`*

Usamos `ThreadPool::new` para criar um novo pool de threads com um número configurável de threads, neste caso quatro. Em seguida, no loop `for`, `pool.execute` tem uma interface semelhante a `thread::spawn`, pois recebe uma closure que o pool deve executar para cada stream. Precisamos implementar `pool.execute` para que ele receba a closure e a entregue a uma thread no pool para execução. Este código ainda não será compilado, mas tentaremos para que o compilador possa nos guiar em como corrigi-lo.

#### Construindo `ThreadPool` Usando Desenvolvimento Orientado ao Compilador

Faça as alterações na Listagem 21-12 em *src/main.rs* e, em seguida, vamos usar os erros do compilador de `cargo check` para conduzir nosso desenvolvimento. Aqui está o primeiro erro que recebemos:

```
$ cargo check
    Checking hello v0.1.0 (file:///projects/hello)
error[E0433]: failed to resolve: use of undeclared type `ThreadPool`
  --> src/main.rs:12:16
   |
12 |     let pool = ThreadPool::new(4);
   |                ^^^^^^^^^^ use of undeclared type `ThreadPool`

For more information about this error, try `rustc --explain E0433`.
error: could not compile `hello` (bin "hello") due to 1 previous error
```

Ótimo! Este erro nos diz que precisamos de um tipo ou módulo `ThreadPool`, então vamos construir um agora. Nossa implementação de `ThreadPool` será independente do tipo de trabalho que nosso servidor web está fazendo. Então, vamos mudar a crate `hello` de uma crate binária para uma crate de biblioteca para conter nossa implementação de `ThreadPool`. Depois de mudarmos para uma crate de biblioteca, também poderíamos usar a biblioteca de pool de threads separada para qualquer trabalho que quisermos fazer usando um pool de threads, não apenas para servir requisições web.

Crie um *src/lib.rs* que contenha o seguinte, que é a definição mais simples de uma struct `ThreadPool` que podemos ter por enquanto:

*Arquivo: src/lib.rs*

```rust
pub struct ThreadPool;
```
Em seguida, edite o arquivo *main.rs* para trazer o `ThreadPool` para o escopo da crate da biblioteca, adicionando o seguinte código ao topo de *src/main.rs*:

*Arquivo: src/main.rs*

```rust
use hello::ThreadPool;
// -- Resto dos imports e código omitidos --
```

Este código ainda não funcionará, mas vamos verificá-lo novamente para obter o próximo erro que precisamos resolver:

```
$ cargo check
    Checking hello v0.1.0 (file:///projects/hello)
error[E0599]: no function or associated item named `new` found for struct `ThreadPool` in the current scope
  --> src/main.rs:13:28
   |
13 |     let pool = ThreadPool::new(4);
   |                            ^^^ function or associated item not found in `ThreadPool`

For more information about this error, try `rustc --explain E0599`.
error: could not compile `hello` (bin "hello") due to 1 previous error
```

Este erro indica que, em seguida, precisamos criar uma função associada chamada `new` para `ThreadPool`. Também sabemos que `new` precisa ter um parâmetro que possa aceitar `4` como argumento e deve retornar uma instância de `ThreadPool`. Vamos implementar a função `new` mais simples que terá essas características:

*Arquivo: src/lib.rs*

```rust
pub struct ThreadPool;

impl ThreadPool {
    pub fn new(size: usize) -> ThreadPool {
        ThreadPool
    }
}
```

Escolhemos `usize` como o tipo do parâmetro `size`, porque sabemos que um número negativo de threads não faz sentido. Também sabemos que usaremos este 4 como o número de elementos em uma coleção de threads, que é para que serve o tipo `usize`, conforme discutido na seção "Tipos Inteiros" do Capítulo 3.

Vamos verificar o código novamente:

```
$ cargo check
    Checking hello v0.1.0 (file:///projects/hello)
error[E0599]: no method named `execute` found for struct `ThreadPool` in the current scope
  --> src/main.rs:18:14
   |
18 |         pool.execute(|| {
   |         -----^^^^^^^ method not found in `ThreadPool`

For more information about this error, try `rustc --explain E0599`.
error: could not compile `hello` (bin "hello") due to 1 previous error
```

Agora o erro ocorre porque não temos um método `execute` em `ThreadPool`. Lembre-se da seção "Criando um Número Finito de Threads" que decidimos que nosso pool de threads deve ter uma interface semelhante a `thread::spawn`. Além disso, implementaremos a função `execute` para que ela receba a closure que recebe e a entregue a uma thread ociosa no pool para execução.

Definiremos o método `execute` em `ThreadPool` para receber uma closure como parâmetro. Lembre-se da seção "Movendo Valores Capturados para Fora da Closure e as Traits `Fn`" no Capítulo 13 que podemos receber closures como parâmetros com três traits diferentes: `Fn`, `FnMut` e `FnOnce`. Precisamos decidir que tipo de closure usar aqui. Sabemos que acabaremos fazendo algo semelhante à implementação de `thread::spawn` da biblioteca padrão, então podemos ver quais limites a assinatura de `thread::spawn` tem em seu parâmetro. A documentação nos mostra o seguinte:

```rust
pub fn spawn<F, T>(f: F) -> JoinHandle<T>
    where
        F: FnOnce() -> T,
        F: Send + 'static,
        T: Send + 'static,
```

O parâmetro de tipo `F` é o que nos interessa aqui; o parâmetro de tipo `T` está relacionado ao valor de retorno, e não estamos preocupados com isso. Podemos ver que `spawn` usa `FnOnce` como o limite de trait em `F`. Isso é provavelmente o que queremos também, porque eventualmente passaremos o argumento que recebemos em `execute` para `spawn`. Podemos ter ainda mais certeza de que `FnOnce` é a trait que queremos usar porque a thread para executar uma requisição só executará a closure dessa requisição uma vez, o que corresponde ao `Once` em `FnOnce`.

O parâmetro de tipo `F` também tem o limite de trait `Send` e o limite de tempo de vida `'static`, que são úteis em nossa situação: precisamos de `Send` para transferir a closure de uma thread para outra e `'static` porque não sabemos quanto tempo a thread levará para executar. Vamos criar um método `execute` em `ThreadPool` que receberá um parâmetro genérico do tipo `F` com esses limites:

*Arquivo: src/lib.rs*

```rust
pub struct ThreadPool;

impl ThreadPool {
    // --snip--
    pub fn new(size: usize) -> ThreadPool {
        ThreadPool
    }

    pub fn execute<F>(&self, f: F)
    where
        F: FnOnce() + Send + 'static,
    {
    }
}
```

Ainda usamos os `()` após `FnOnce` porque este `FnOnce` representa uma closure que não recebe parâmetros e retorna o tipo unitário `()`. Assim como nas definições de função, o tipo de retorno pode ser omitido da assinatura, mas mesmo que não tenhamos parâmetros, ainda precisamos dos parênteses.

Novamente, esta é a implementação mais simples do método `execute`: ele não faz nada, mas estamos tentando apenas fazer nosso código compilar. Vamos verificá-lo novamente:

```
$ cargo check
    Checking hello v0.1.0 (file:///projects/hello)
    Finished dev [unoptimized + debuginfo] target(s) in 0.24s
```

Compila! Mas observe que se você tentar `cargo run` e fizer uma requisição no navegador, você verá os erros no navegador que vimos no início do capítulo. Nossa biblioteca ainda não está chamando a closure passada para `execute`!

Nota: Um ditado que você pode ouvir sobre linguagens com compiladores estritos, como Haskell e Rust, é "se o código compila, ele funciona". Mas este ditado não é universalmente verdadeiro. Nosso projeto compila, mas não faz absolutamente nada! Se estivéssemos construindo um projeto real e completo, este seria um bom momento para começar a escrever testes unitários para verificar se o código compila *e* tem o comportamento que queremos.

Considere: o que seria diferente aqui se fôssemos executar um *future* em vez de uma closure?

#### Validando o Número de Threads em `new`

Não estamos fazendo nada com os parâmetros para `new` e `execute`. Vamos implementar os corpos dessas funções com o comportamento que queremos. Para começar, vamos pensar em `new`. Anteriormente, escolhemos um tipo não assinado para o parâmetro `size`, porque um pool com um número negativo de threads não faz sentido. No entanto, um pool com zero threads também não faz sentido, mas zero é um `usize` perfeitamente válido. Adicionaremos código para verificar se `size` é maior que zero antes de retornar uma instância de `ThreadPool` e fazer o programa entrar em pânico se receber um zero usando a macro `assert!`, como mostrado na Listagem 21-13.

*Arquivo: src/lib.rs*

```rust
pub struct ThreadPool;

impl ThreadPool {
    /// Cria um novo ThreadPool.
    ///
    /// O tamanho é o número de threads no pool.
    ///
    /// # Panics
    ///
    /// A função `new` entrará em pânico se o tamanho for zero.
    pub fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        ThreadPool
    }

    // --snip--
    pub fn execute<F>(&self, f: F)
    where
        F: FnOnce() + Send + 'static,
    {
    }
}
```

*Listagem 21-13: Implementando `ThreadPool::new` para entrar em pânico se `size` for zero*

Também adicionamos alguma documentação para nosso `ThreadPool` com comentários de documentação. Observe que seguimos as boas práticas de documentação adicionando uma seção que destaca as situações em que nossa função pode entrar em pânico, conforme discutido no Capítulo 14. Tente executar `cargo doc --open` e clicar na struct `ThreadPool` para ver como ficam os documentos gerados para `new`!

Em vez de adicionar a macro `assert!` como fizemos aqui, poderíamos mudar `new` para `build` e retornar um `Result` como fizemos com `Config::build` no projeto de E/S na Listagem 12-9. Mas decidimos, neste caso, que tentar criar um pool de threads sem nenhuma thread deve ser um erro irrecuperável. Se você estiver se sentindo ambicioso, tente escrever uma função chamada `build` com a seguinte assinatura para comparar com a função `new`:

```rust
pub fn build(size: usize) -> Result<ThreadPool, PoolCreationError> {
```

#### Criando Espaço para Armazenar as Threads

Agora que temos uma maneira de saber que temos um número válido de threads para armazenar no pool, podemos criar essas threads e armazená-las na struct `ThreadPool` antes de retornar a struct. Mas como "armazenamos" uma thread? Vamos dar outra olhada na assinatura de `thread::spawn`:

```rust
pub fn spawn<F, T>(f: F) -> JoinHandle<T>
    where
        F: FnOnce() -> T,
        F: Send + 'static,
        T: Send + 'static,
```

A função `spawn` retorna um `JoinHandle<T>`, onde `T` é o tipo que a closure retorna. Vamos tentar usar `JoinHandle` também e ver o que acontece. No nosso caso, as closures que estamos passando para o pool de threads lidarão com a conexão e não retornarão nada, então `T` será o tipo unitário `()`.

O código na Listagem 21-14 será compilado, mas ainda não cria nenhuma thread. Mudamos a definição de `ThreadPool` para conter um vetor de instâncias de `thread::JoinHandle<()>`, inicializamos o vetor com uma capacidade de `size`, configuramos um loop `for` que executará algum código para criar as threads e retornamos uma instância de `ThreadPool` contendo-as.

*Arquivo: src/lib.rs*

```rust
use std::thread;

pub struct ThreadPool {
    threads: Vec<thread::JoinHandle<()>>,
}

impl ThreadPool {
    // --snip--
    /// Cria um novo ThreadPool.
    ///
    /// O tamanho é o número de threads no pool.
    ///
    /// # Panics
    ///
    /// A função `new` entrará em pânico se o tamanho for zero.
    pub fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        let mut threads = Vec::with_capacity(size);

        for _ in 0..size {
            // cria algumas threads e as armazena no vetor
        }

        ThreadPool { threads }
    }
    // --snip--
    pub fn execute<F>(&self, f: F)
    where
        F: FnOnce() + Send + 'static,
 {
    }
}
```
*Listagem 21-14: Criando um vetor para `ThreadPool` conter as threads*

Trouxemos `std::thread` para o escopo na crate da biblioteca, porque estamos usando `thread::JoinHandle` como o tipo dos itens no vetor em `ThreadPool`.

Uma vez que um tamanho válido é recebido, nosso `ThreadPool` cria um novo vetor que pode conter itens `size`. A função `with_capacity` executa a mesma tarefa que `Vec::new`, mas com uma diferença importante: ela pré-aloca espaço no vetor. Como sabemos que precisamos armazenar elementos `size` no vetor, fazer essa alocação antecipadamente é um pouco mais eficiente do que usar `Vec::new`, que se redimensiona à medida que os elementos são inseridos.

Quando você executar `cargo check` novamente, ele deverá ter sucesso.

#### Uma Struct `Worker` Responsável por Enviar Código do `ThreadPool` para uma Thread

Deixamos um comentário no loop `for` na Listagem 21-14 sobre a criação de threads. Aqui, veremos como realmente criamos threads. A biblioteca padrão fornece `thread::spawn` como uma forma de criar threads, e `thread::spawn` espera receber algum código que a thread deve executar assim que a thread for criada. No entanto, no nosso caso, queremos criar as threads e fazê-las *esperar* pelo código que enviaremos mais tarde. A implementação de threads da biblioteca padrão não inclui nenhuma maneira de fazer isso; temos que implementá-la manualmente.

Implementaremos esse comportamento introduzindo uma nova estrutura de dados entre o `ThreadPool` e as threads que gerenciará esse novo comportamento. Chamaremos essa estrutura de dados de *Worker*, que é um termo comum em implementações de pooling. O Worker pega o código que precisa ser executado e executa o código na thread do Worker. Pense nas pessoas que trabalham na cozinha de um restaurante: os trabalhadores esperam até que os pedidos cheguem dos clientes e, em seguida, são responsáveis por receber esses pedidos e atendê-los.

Em vez de armazenar um vetor de instâncias de `JoinHandle<()>` no pool de threads, armazenaremos instâncias da struct `Worker`. Cada `Worker` armazenará uma única instância de `JoinHandle<()>. Em seguida, implementaremos um método em `Worker` que receberá uma closure de código para executar e a enviará para a thread já em execução para execução. Também daremos a cada trabalhador um `id` para que possamos distinguir entre os diferentes trabalhadores no pool ao registrar ou depurar.

Aqui está o novo processo que acontecerá quando criarmos um `ThreadPool`. Implementaremos o código que envia a closure para a thread depois de termos o `Worker` configurado desta forma:

*   Definir uma struct `Worker` que contém um `id` e um `JoinHandle<()>`.
*   Alterar `ThreadPool` para conter um vetor de instâncias de `Worker`.
*   Definir uma função `Worker::new` que recebe um número `id` e retorna uma instância de `Worker` que contém o `id` e uma thread gerada com uma closure vazia.
*   Em `ThreadPool::new`, usar o contador do loop `for` para gerar um `id`, criar um novo `Worker` com esse `id` e armazenar o trabalhador no vetor.

Se você estiver pronto para um desafio, tente implementar essas alterações por conta própria antes de olhar o código na Listagem 21-15.

Pronto? Aqui está a Listagem 21-15 com uma maneira de fazer as modificações anteriores.

*Arquivo: src/lib.rs*

```rust
use std::thread;

pub struct ThreadPool {
    workers: Vec<Worker>,
}

impl ThreadPool {
    // --snip--
    /// Create a new ThreadPool.
    ///
    /// The size is the number of threads in the pool.
    ///
    /// # Panics
    ///
    /// The `new` function will panic if the size is zero.
    pub fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        let mut workers = Vec::with_capacity(size);

        for id in 0..size {
            workers.push(Worker::new(id));
        }

        ThreadPool { workers }
    }
    // --snip--

    pub fn execute<F>(&self, f: F)
    where
        F: FnOnce() + Send + 'static,
    {
    }
}

struct Worker {
    id: usize,
    thread: thread::JoinHandle<()>,
}

impl Worker {
    fn new(id: usize) -> Worker {
        let thread = thread::spawn(|| {});

        Worker { id, thread }
    }
}
```
*Listagem 21-15: Modificando `ThreadPool` para conter instâncias de `Worker` em vez de conter threads diretamente*

Mudamos o nome do campo em `ThreadPool` de `threads` para `workers` porque agora ele contém instâncias de `Worker` em vez de instâncias de `JoinHandle<()>. Usamos o contador no loop `for` como um argumento para `Worker::new` e armazenamos cada novo `Worker` no vetor chamado `workers`.

O código externo (como nosso servidor em *src/main.rs*) não precisa saber os detalhes de implementação sobre o uso de uma struct `Worker` dentro de `ThreadPool`, então tornamos a struct `Worker` e sua função `new` privadas. A função `Worker::new` usa o `id` que fornecemos e armazena uma instância de `JoinHandle<()>` que é criada gerando uma nova thread usando uma closure vazia.

Nota: Se o sistema operacional não puder criar uma thread porque não há recursos de sistema suficientes, `thread::spawn` entrará em pânico. Isso fará com que todo o nosso servidor entre em pânico, mesmo que a criação de algumas threads possa ter sucesso. Para simplificar, esse comportamento é bom, mas em uma implementação de pool de threads de produção, você provavelmente gostaria de usar `std::thread::Builder` e seu método `spawn` que retorna `Result`.

Este código será compilado e armazenará o número de instâncias de `Worker` que especificamos como um argumento para `ThreadPool::new`. Mas *ainda* não estamos processando a closure que recebemos em `execute`. Vamos ver como fazer isso em seguida.

#### Enviando Requisições para Threads via Canais

O próximo problema que abordaremos é que as closures dadas a `thread::spawn` não fazem absolutamente nada. Atualmente, obtemos a closure que queremos executar no método `execute`. Mas precisamos dar a `thread::spawn` uma closure para executar quando criamos cada `Worker` durante a criação do `ThreadPool`.

Queremos que as structs `Worker` que acabamos de criar busquem o código a ser executado em uma fila mantida no `ThreadPool` e enviem esse código para sua thread para execução.

Os canais que aprendemos no Capítulo 16 — uma maneira simples de comunicação entre duas threads — seriam perfeitos para este caso de uso. Usaremos um canal para funcionar como a fila de tarefas, e `execute` enviará uma tarefa do `ThreadPool` para as instâncias de `Worker`, que enviarão a tarefa para sua thread. Aqui está o plano:

*   O `ThreadPool` criará um canal e se manterá no remetente (sender).
*   Cada `Worker` se manterá no receptor (receiver).
*   Criaremos uma nova struct `Job` que conterá as closures que queremos enviar pelo canal.
*   O método `execute` enviará a tarefa que deseja executar através do remetente.
*   Em sua thread, o `Worker` fará um loop em seu receptor e executará as closures de quaisquer tarefas que receber.

Vamos começar criando um canal em `ThreadPool::new` e mantendo o remetente na instância de `ThreadPool`, como mostrado na Listagem 21-16. A struct `Job` não contém nada por enquanto, mas será o tipo de item que estamos enviando pelo canal.

*Arquivo: src/lib.rs*

```rust
use std::{sync::mpsc, thread};

pub struct ThreadPool {
    workers: Vec<Worker>,
    sender: mpsc::Sender<Job>,
}

struct Job;

impl ThreadPool {
    // --snip--
    /// Create a new ThreadPool.
    ///
    /// The size is the number of threads in the pool.
    ///
    /// # Panics
    ///
    /// The `new` function will panic if the size is zero.
    pub fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        let (sender, receiver) = mpsc::channel();

        let mut workers = Vec::with_capacity(size);

        for id in 0..size {
            workers.push(Worker::new(id));
        }

        ThreadPool { workers, sender }
    }
    // --snip--

    pub fn execute<F>(&self, f: F)
    where
        F: FnOnce() + Send + 'static,
    {
    }
}
//--snip--
struct Worker {
    id: usize,
    thread: thread::JoinHandle<()>,
}

impl Worker {
    fn new(id: usize) -> Worker {
        let thread = thread::spawn(|| {});

        Worker { id, thread }
    }
}
```
*Listagem 21-16: Modificando `ThreadPool` para armazenar o remetente de um canal que transmite instâncias de `Job`.*

Em `ThreadPool::new`, criamos nosso novo canal e fazemos o pool manter o remetente. Isso será compilado com sucesso.

Vamos tentar passar um receptor do canal para cada trabalhador conforme o pool de threads cria o canal. Sabemos que queremos usar o receptor na thread que os trabalhadores geram, então referenciaremos o parâmetro `receiver` na closure. O código na Listagem 21-17 ainda não será compilado.

*Arquivo: src/lib.rs*




```rust
use std::{sync::mpsc, thread};

pub struct ThreadPool {
    workers: Vec<Worker>,
    sender: mpsc::Sender<Job>,
}

struct Job;

impl ThreadPool {
    // --snip--
    /// Create a new ThreadPool.
    ///
    /// The size is the number of threads in the pool.
    ///
    /// # Panics
    ///
    /// The `new` function will panic if the size is zero.
    pub fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        let (sender, receiver) = mpsc::channel();

        let mut workers = Vec::with_capacity(size);

        for id in 0..size {
            workers.push(Worker::new(id, receiver));
        }

        ThreadPool { workers, sender }
    }
    // --snip--

    pub fn execute<F>(&self, f: F)
    where
        F: FnOnce() + Send + 'static,
    {
    }
}

// --snip--

struct Worker {
    id: usize,
    thread: thread::JoinHandle<()>,
}

impl Worker {
    fn new(id: usize, receiver: mpsc::Receiver<Job>) -> Worker {
        let thread = thread::spawn(|| {
            receiver;
        });

        Worker { id, thread }
    }
}
```

*Listagem 21-17: Passando o receptor para os trabalhadores*

Fizemos algumas pequenas e diretas alterações: passamos o receptor para `Worker::new` e, em seguida, o usamos dentro da closure.

Quando tentamos verificar este código, obtemos este erro:

```
$ cargo check
    Checking hello v0.1.0 (file:///projects/hello)
error[E0382]: use of moved value: `receiver`
  --> src/lib.rs:26:42
   |
21 |         let (sender, receiver) = mpsc::channel();
   |                      -------- move occurs because `receiver` has type `std::sync::mpsc::Receiver<Job>`, which does not implement the `Copy` trait
...
25 |         for id in 0..size {
   |         ----------------- inside of this loop
26 |             workers.push(Worker::new(id, receiver));
   |                                          ^^^^^^^^ value moved here, in previous iteration of loop
   |
note: consider changing this parameter type in method `new` to borrow instead if owning the value isn't necessary
  --> src/lib.rs:47:33
   |
47 |     fn new(id: usize, receiver: mpsc::Receiver<Job>) -> Worker {
   |        --- in this method       ^^^^^^^^^^^^^^^^^^^ this parameter takes ownership of the value
help: consider moving the expression out of the loop so it is only moved once
   |
25 ~         let mut value = Worker::new(id, receiver);
26 ~         for id in 0..size {
27 ~             workers.push(value);
   |

For more information about this error, try `rustc --explain E0382`.
error: could not compile `hello` (lib) due to 1 previous error
```

O código está tentando passar `receiver` para múltiplas instâncias de `Worker`. Isso não funcionará, como você deve se lembrar do Capítulo 16: a implementação de canal que o Rust fornece é de múltiplos *produtores* e único *consumidor*. Isso significa que não podemos simplesmente clonar a extremidade consumidora do canal para corrigir esse código. Também não queremos enviar uma mensagem várias vezes para vários consumidores; queremos uma lista de mensagens com vários trabalhadores de forma que cada mensagem seja processada uma vez.

Além disso, retirar uma tarefa da fila do canal envolve a mutação de `receiver`, então as threads precisam de uma maneira segura de compartilhar e modificar `receiver`; caso contrário, poderíamos ter condições de corrida (conforme abordado no Capítulo 16).

Lembre-se dos smart pointers thread-safe discutidos no Capítulo 16: para compartilhar a propriedade entre várias threads e permitir que as threads modifiquem o valor, precisamos usar `Arc<Mutex<T>>`. O tipo `Arc` permitirá que vários trabalhadores possuam o receptor, e `Mutex` garantirá que apenas um trabalhador receba uma tarefa do receptor por vez. A Listagem 21-18 mostra as alterações que precisamos fazer.

*Arquivo: src/lib.rs*

```rust
use std::{
    sync::{mpsc, Arc, Mutex},
    thread,
};
// --snip--

pub struct ThreadPool {
    workers: Vec<Worker>,
    sender: mpsc::Sender<Job>,
}

struct Job;

impl ThreadPool {
    // --snip--
    /// Create a new ThreadPool.
    ///
    /// The size is the number of threads in the pool.
    ///
    /// # Panics
    ///
    /// The `new` function will panic if the size is zero.
    pub fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        let (sender, receiver) = mpsc::channel();

        let receiver = Arc::new(Mutex::new(receiver));

        let mut workers = Vec::with_capacity(size);

        for id in 0..size {
            workers.push(Worker::new(id, Arc::clone(&receiver)));
        }

        ThreadPool { workers, sender }
    }

 // --snip--

    pub fn execute<F>(&self, f: F)
    where
        F: FnOnce() + Send + 'static,
    {
    }
}

// --snip--

struct Worker {
    id: usize,
    thread: thread::JoinHandle<()>,
}

impl Worker {
    fn new(id: usize, receiver: Arc<Mutex<mpsc::Receiver<Job>>>) -> Worker {
        // --snip--
        let thread = thread::spawn(|| {
            receiver;
        });

        Worker { id, thread }
    }
}
```
*Listagem 21-18: Compartilhando o receptor entre os trabalhadores usando `Arc` e `Mutex`*

Em `ThreadPool::new`, colocamos o receptor em um `Arc` e um `Mutex`. Para cada novo trabalhador, clonamos o `Arc` para aumentar a contagem de referência para que os trabalhadores possam compartilhar a propriedade do receptor.

Com essas alterações, o código compila! Estamos chegando lá!

#### Implementando o Método `execute`

Vamos finalmente implementar o método `execute` em `ThreadPool`. Também mudaremos `Job` de uma struct para um alias de tipo para um objeto trait que contém o tipo de closure que `execute` recebe. Conforme discutido na seção "Criando Sinônimos de Tipo com Aliases de Tipo" do Capítulo 20, aliases de tipo nos permitem tornar tipos longos mais curtos para facilitar o uso. Veja a Listagem 21-19.

*Arquivo: src/lib.rs*

```rust
use std::{
    sync::{mpsc, Arc, Mutex},
    thread,
};

pub struct ThreadPool {
    workers: Vec<Worker>,
    sender: mpsc::Sender<Job>,
}

// --snip--

type Job = Box<dyn FnOnce() + Send + 'static>;

impl ThreadPool {
    // --snip--
    /// Create a new ThreadPool.
    ///
    /// The size is the number of threads in the pool.
    ///
    /// # Panics
    ///
    /// The `new` function will panic if the size is zero.
    pub fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        let (sender, receiver) = mpsc::channel();

        let receiver = Arc::new(Mutex::new(receiver));

        let mut workers = Vec::with_capacity(size);

        for id in 0..size {
            workers.push(Worker::new(id, Arc::clone(&receiver)));
        }

        ThreadPool { workers, sender }
    }

    pub fn execute<F>(&self, f: F)
    where
        F: FnOnce() + Send + 'static,
    {
        let job = Box::new(f);

        self.sender.send(job).unwrap();
    }
}

// --snip--
struct Worker {
    id: usize,
    thread: thread::JoinHandle<()>,
}

impl Worker {
    fn new(id: usize, receiver: Arc<Mutex<mpsc::Receiver<Job>>>) -> Worker {
        let thread = thread::spawn(|| {
            receiver;
        });

        Worker { id, thread }
    }
}
```
*Listagem 21-19: Criando um alias de tipo `Job` para um `Box` que contém cada closure e, em seguida, enviando a tarefa pelo canal*

Depois de criar uma nova instância de `Job` usando a closure que recebemos em `execute`, enviamos essa tarefa pela extremidade de envio do canal. Estamos chamando `unwrap` em `send` para o caso de o envio falhar. Isso pode acontecer se, por exemplo, pararmos todas as nossas threads de executar, o que significa que a extremidade receptora parou de receber novas mensagens. No momento, não podemos parar nossas threads de executar: nossas threads continuam executando enquanto o pool existir. A razão pela qual usamos `unwrap` é que sabemos que o caso de falha não acontecerá, mas o compilador não sabe disso.

Mas ainda não terminamos! No trabalhador, nossa closure sendo passada para `thread::spawn` ainda apenas *referencia* a extremidade receptora do canal. Em vez disso, precisamos que a closure faça um loop para sempre, pedindo à extremidade receptora do canal uma tarefa e executando a tarefa quando ela receber uma. Vamos fazer a alteração mostrada na Listagem 21-20 para `Worker::new`.

*Arquivo: src/lib.rs*

```rust
use std::{
    sync::{mpsc, Arc, Mutex},
    thread,
};

pub struct ThreadPool {
    workers: Vec<Worker>,
    sender: Option<mpsc::Sender<Job>>,
}

type Job = Box<dyn FnOnce() + Send + 'static>;

impl ThreadPool {
    /// Create a new ThreadPool.
    ///
    /// The size is the number of threads in the pool.
    ///
    /// # Panics
    ///
    /// The `new` function will panic if the size is zero.
    pub fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        let (sender, receiver) = mpsc::channel();

        let receiver = Arc::new(Mutex::new(receiver));

        let mut workers = Vec::with_capacity(size);

        for id in 0..size {
            workers.push(Worker::new(id, Arc::clone(&receiver)));
        }

        ThreadPool { workers, sender:Some(sender) }
    }

    pub fn execute<F>(&self, f: F)
    where
        F: FnOnce() + Send + 'static,
    {
        let job = Box::new(f);

        self.sender.as_ref().unwrap().send(job).unwrap();
    }
}

struct Worker {
    id: usize,
    thread: thread::JoinHandle<()>,
}

// --snip--

impl Worker {
    fn new(id: usize, receiver: Arc<Mutex<mpsc::Receiver<Job>>>) -> Worker {
        let thread = thread::spawn(move || loop {
            let job = receiver.lock().unwrap().recv().unwrap();

            println!("Worker {id} got a job; executing.");

            job();
        });

        Worker { id, thread }
    }
}
```
*Listagem 21-20: Recebendo e executando as tarefas na thread do trabalhador*

Aqui, primeiro chamamos `lock` no `receiver` para adquirir o mutex e, em seguida, chamamos `unwrap` para entrar em pânico em quaisquer erros. Adquirir um bloqueio pode falhar se o mutex estiver em um estado *envenenado*, o que pode acontecer se alguma outra thread entrar em pânico enquanto mantinha o bloqueio em vez de liberar o bloqueio. Nesta situação, chamar `unwrap` para fazer esta thread entrar em pânico é a ação correta a ser tomada. Sinta-se à vontade para alterar este `unwrap` para um `expect` com uma mensagem de erro que seja significativa para você.

Se obtivermos o bloqueio no mutex, chamamos `recv` para receber um `Job` do canal. Um `unwrap` final passa por quaisquer erros aqui também, que podem ocorrer se a thread que detém o remetente for encerrada, semelhante a como o método `send` retorna `Err` se o receptor for encerrado.

A chamada para `recv` bloqueia, então, se ainda não houver uma tarefa, a thread atual esperará até que uma tarefa fique disponível. O `Mutex<T>` garante que apenas uma thread `Worker` por vez esteja tentando solicitar uma tarefa.  E o sender se tornou uma opção, e estamos realizando um unwrap seguro

Nosso pool de threads agora está em um estado de funcionamento! Dê um `cargo run` e faça algumas requisições:

```
$ cargo run
   Compiling hello v0.1.0 (file:///projects/hello)
warning: field `workers` is never read
 --> src/lib.rs:7:5
  |
6 | pub struct ThreadPool {
  |            ---------- field in this struct
7 |     workers: Vec<Worker>,
  |     ^^^^^^^
  |
  = note: #[warn(dead_code)] on by default

warning: fields `id` and `thread` are never read
  --> src/lib.rs:48:5
   |
47 | struct Worker {
   |        ------ fields in this struct
48 |     id: usize,
   |     ^^
49 |     thread: thread::JoinHandle<()>,
   |     ^^^^^^

warning: `hello` (lib) generated 2 warnings
    Finished dev [unoptimized + debuginfo] target(s) in 4.91s
     Running `target/debug/hello`
Worker 0 got a job; executing.
Worker 2 got a job; executing.
Worker 1 got a job; executing.
Worker 3 got a job; executing.
Worker 0 got a job; executing.
Worker 2 got a job; executing.
Worker 1 got a job; executing.
Worker 3 got a job; executing.
Worker 0 got a job; executing.
Worker 2 got a job; executing.
```

Sucesso! Agora temos um pool de threads que executa conexões de forma assíncrona. Nunca há mais de quatro threads criadas, então nosso sistema não ficará sobrecarregado se o servidor receber muitas requisições. Se fizermos uma requisição para */sleep*, o servidor poderá atender outras requisições fazendo com que outra thread as execute.

Nota: Se você abrir */sleep* em várias janelas do navegador simultaneamente, elas podem carregar uma de cada vez em intervalos de 5 segundos. Alguns navegadores web executam várias instâncias da mesma requisição sequencialmente por motivos de cache. Essa limitação não é causada pelo nosso servidor web.

Este é um bom momento para fazer uma pausa e considerar como o código nas Listagens 21-18, 21-19 e 21-20 seria diferente se estivéssemos usando futures em vez de uma closure para o trabalho a ser feito. Quais tipos mudariam? Como as assinaturas dos métodos seriam diferentes, se é que seriam? Quais partes do código permaneceriam as mesmas?

Depois de aprender sobre o loop `while let` nos Capítulos 17 e 18, você pode estar se perguntando por que não escrevemos o código da thread do trabalhador como mostrado na Listagem 21-21.

*Arquivo: src/lib.rs*

```rust
use std::{
    sync::{mpsc, Arc, Mutex},
    thread,
};

pub struct ThreadPool {
    workers: Vec<Worker>,
    sender: mpsc::Sender<Job>,
}

type Job = Box<dyn FnOnce() + Send + 'static>;

impl ThreadPool {
    /// Create a new ThreadPool.
    ///
    /// The size is the number of threads in the pool.
    ///
    /// # Panics
    ///
    /// The `new` function will panic if the size is zero.
    pub fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        let (sender, receiver) = mpsc::channel();

        let receiver = Arc::new(Mutex::new(receiver));

        let mut workers = Vec::with_capacity(size);

        for id in 0..size {
            workers.push(Worker::new(id, Arc::clone(&receiver)));
        }

        ThreadPool { workers, sender }
    }

    pub fn execute<F>(&self, f: F)
    where
        F: FnOnce() + Send + 'static,
    {
        let job = Box::new(f);

        self.sender.send(job).unwrap();
    }
}

struct Worker {
    id: usize,
    thread: thread::JoinHandle<()>,
}
// --snip--

impl Worker {
    fn new(id: usize, receiver: Arc<Mutex<mpsc::Receiver<Job>>>) -> Worker {
        let thread = thread::spawn(move || {
            while let Ok(job) = receiver.lock().unwrap().recv() {
                println!("Worker {id} got a job; executing.");

                job();
            }
        });

        Worker { id, thread }
    }
}
```

*Listagem 21-21: Uma implementação alternativa de `Worker::new` usando `while let`*

Este código compila e executa, mas não resulta no comportamento de threading desejado: uma requisição lenta ainda fará com que outras requisições esperem para serem processadas. A razão é um tanto sutil: a struct `Mutex` não tem um método público `unlock` porque a propriedade do bloqueio é baseada no tempo de vida de `MutexGuard<T>` dentro de `LockResult<MutexGuard<T>>` que o método `lock` retorna. Em tempo de compilação, o verificador de empréstimo pode então impor a regra de que um recurso protegido por um `Mutex` não pode ser acessado a menos que mantenhamos o bloqueio. No entanto, essa implementação também pode resultar no bloqueio sendo mantido por mais tempo do que o pretendido se não estivermos atentos ao tempo de vida de `MutexGuard<T>`.

O código na Listagem 21-20 que usa `let job = receiver.lock().unwrap().recv().unwrap();` funciona porque com `let`, quaisquer valores temporários usados na expressão no lado direito do sinal de igual são imediatamente descartados quando a instrução `let` termina. No entanto, `while let` (e `if let` e `match`) não descarta valores temporários até o final do bloco associado. Na Listagem 21-21, o bloqueio permanece ativo durante a duração da chamada para `job()`, o que significa que outros trabalhadores não podem receber tarefas.